{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df33ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gcinizwe/miniconda3/envs/venv_dpd_py38/bin/pip\r\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9f42a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gcinizwe/miniconda3/envs/venv_dpd_py38/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9042b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\tandroid2\t    intersection.py  project_titles.pkl\r\n",
      "android.zip\tfiltered_repos.zip  new_directory    train_data\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3749bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import fnmatch\n",
    "import shutil\n",
    "\n",
    "\n",
    "def delete_dir(path):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "\n",
    "def read_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Load the serialized object from the file\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "\n",
    "folder_titles = read_pickle_file(\"project_titles.pkl\")\n",
    "\n",
    "# Total items: 919\n",
    "stripped_titles = [os.path.normpath(t).split(os.sep)[-1] for t in folder_titles]\n",
    "\n",
    "assert len(stripped_titles) == 919\n",
    "\n",
    "\n",
    "def find_duplicates():\n",
    "    duplicates = 0\n",
    "\n",
    "    for dirname in os.listdir(\"./android2\"):\n",
    "\n",
    "        if dirname in stripped_titles:\n",
    "            # Uncomment the following code to delete the files\n",
    "            # full_path=folder_titles[stripped_titles.index(dirname)]\n",
    "            # delete_dir(full_path)\n",
    "            duplicates += 1\n",
    "\n",
    "    print(\"dupl: \", duplicates)\n",
    "\n",
    "\n",
    "# for root, dirnames_parent, filenames in os.walk('./new_directory'):\n",
    "#     for dirname in dirnames_parent:\n",
    "#         dir_path = './new_directory/' + f'{dirname}'\n",
    "#         src_path = find_src_path(dir_path)\n",
    "#         get_java_paths()\n",
    "        # for _, dirnames, filenames in os.walk(dir_path):\n",
    "        #     breakpoint()\n",
    "        #     if not fnmatch.fnmatch(filename, '*.java'):\n",
    "        #         file_path = os.path.join(root, filename)\n",
    "        #         breakpoint()\n",
    "        #         os.remove(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d5b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30f55e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC not FOUND\n",
      "SRC not FOUND\n",
      "SRC not FOUND\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_src_path(android_dir):\n",
    "    for root, dirs, files in os.walk(android_dir):\n",
    "        if (root.endswith('src')):\n",
    "            return root\n",
    "\n",
    "\n",
    "def find_files_recursively(src_dir: str):\n",
    "    \"\"\"find all .java files recursively from src dir\"\"\"\n",
    "    java_files = []\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.java'):\n",
    "                java_files.append(root + '/' + file)\n",
    "    return java_files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_java_paths(base) -> pd.DataFrame:\n",
    "    all_dirs = []\n",
    "    all_java_paths = []\n",
    "    for android_dir in os.listdir(base):\n",
    "        src = find_src_path(base / android_dir)\n",
    "        if src:\n",
    "            java_paths = find_files_recursively(src)\n",
    "            all_dirs.extend(len(java_paths) * [android_dir])\n",
    "            all_java_paths.extend(java_paths)\n",
    "        else:\n",
    "            print(\"SRC not FOUND\")\n",
    "    df = pd.DataFrame({\"projects\": all_dirs, \"java_paths\": all_java_paths})\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df_all = get_java_paths(Path('./new_directory'))\n",
    "breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e454fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14400, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b495bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projects</th>\n",
       "      <th>java_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adblockplusandroid</td>\n",
       "      <td>new_directory/adblockplusandroid/src/org/liter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adblockplusandroid</td>\n",
       "      <td>new_directory/adblockplusandroid/src/org/apach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adblockplusandroid</td>\n",
       "      <td>new_directory/adblockplusandroid/src/org/apach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adblockplusandroid</td>\n",
       "      <td>new_directory/adblockplusandroid/src/org/apach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adblockplusandroid</td>\n",
       "      <td>new_directory/adblockplusandroid/src/org/adblo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14395</th>\n",
       "      <td>ArcLayout</td>\n",
       "      <td>new_directory/ArcLayout/library/src/main/java/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14396</th>\n",
       "      <td>ArcLayout</td>\n",
       "      <td>new_directory/ArcLayout/library/src/main/java/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14397</th>\n",
       "      <td>ArcLayout</td>\n",
       "      <td>new_directory/ArcLayout/library/src/main/java/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14398</th>\n",
       "      <td>ArcLayout</td>\n",
       "      <td>new_directory/ArcLayout/library/src/main/java/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14399</th>\n",
       "      <td>ArcLayout</td>\n",
       "      <td>new_directory/ArcLayout/library/src/androidTes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 projects                                         java_paths\n",
       "0      adblockplusandroid  new_directory/adblockplusandroid/src/org/liter...\n",
       "1      adblockplusandroid  new_directory/adblockplusandroid/src/org/apach...\n",
       "2      adblockplusandroid  new_directory/adblockplusandroid/src/org/apach...\n",
       "3      adblockplusandroid  new_directory/adblockplusandroid/src/org/apach...\n",
       "4      adblockplusandroid  new_directory/adblockplusandroid/src/org/adblo...\n",
       "...                   ...                                                ...\n",
       "14395           ArcLayout  new_directory/ArcLayout/library/src/main/java/...\n",
       "14396           ArcLayout  new_directory/ArcLayout/library/src/main/java/...\n",
       "14397           ArcLayout  new_directory/ArcLayout/library/src/main/java/...\n",
       "14398           ArcLayout  new_directory/ArcLayout/library/src/main/java/...\n",
       "14399           ArcLayout  new_directory/ArcLayout/library/src/androidTes...\n",
       "\n",
       "[14400 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7197fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_code(path):\n",
    "    with open(path, 'r', encoding='cp1252', errors='ignore') as p:\n",
    "        return p.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1aaa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_code = []\n",
    "\n",
    "for _, (project, path) in df_all.iterrows():\n",
    "    all_code.append(read_code(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bea1e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd259009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1040it [00:29, 34.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "t_path = Path('./android2')\n",
    "\n",
    "all_code_t = []\n",
    "empty_android_projects = 0\n",
    "\n",
    "\n",
    "for i, android_dir in tqdm(enumerate(os.listdir(t_path))):\n",
    "    data_dir = t_path / android_dir\n",
    "    java_files = os.listdir(data_dir)\n",
    "    if not java_files:\n",
    "        empty_android_projects += 1\n",
    "    else:\n",
    "        for jfile in java_files:\n",
    "            jfull_path = data_dir / jfile\n",
    "            all_code_t.append(read_code(jfull_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f825f536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The number of projects that are empty: 186'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The number of projects that are empty: %d\" % empty_android_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b385fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14400, 140653)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_code), len(all_code_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c58eafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155053"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_java = all_code + all_code_t\n",
    "len(all_data_java)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6d8ccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77526\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set the random seed to ensure the same random samples every time\n",
    "random.seed(42)\n",
    "\n",
    "# Calculate 50% of the data size\n",
    "sample_size = int(0.5 * len(all_data_java))\n",
    "\n",
    "# Sample 10% of the data\n",
    "random_sample_java = random.sample(all_data_java, sample_size)\n",
    "\n",
    "print(len(random_sample_java))\n",
    "all_data_java = random_sample_java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "994c256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lines(path, data):\n",
    "    with open(path, 'a') as f:\n",
    "        for code in data:  # all_code[:len(all_code) - percent_10]:\n",
    "            f.write(code.replace('\\n', '\\\\n'))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ea5dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77526"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_java)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e10f8ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_10 = int(len(all_data_java) * 0.1)\n",
    "percent_10 < len(all_data_java)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fde42c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7752"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_java[-percent_10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78fe00e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Classification Head - BigBird.ipynb'   coach_repos_zip.zip\r\n",
      " LM_Tuning_BigBird.ipynb\t        coach_repos_zip.zipranvcygstmp\r\n",
      " __MACOSX\t\t\t        filtered_repos.zip\r\n",
      " android.zip\t\t\t        intersection.py\r\n",
      " android2\t\t\t        logs_output\r\n",
      " artifacts\t\t\t        model_output\r\n",
      " bigbird_tuning.py\t\t        new_directory\r\n",
      " catboost_info\t\t\t        project_titles.pkl\r\n",
      " ck_results_of_repos.csv\t        train_data\r\n",
      " coach_repos_zip\t\t        train_data_sample_50percent\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb2e35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -f train_data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1abb664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -f train_data_sample_50percent/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad9db4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_lines(\"train_data_sample_50percent/train_data.txt\", all_data_java[:len(all_data_java) - percent_10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cfa1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_lines(\"train_data_sample_50percent/test_data.txt\", all_data_java[-percent_10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5433bf",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc22d17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.24.0'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f0b448d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.24.0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1026631b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "204cff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BigBirdModel\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = 'google/bigbird-roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5919e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_len(code_input):\n",
    "    inputs = tokenizer(code_input , return_tensors=\"pt\")\n",
    "    return inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a09a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Classification Head - BigBird.ipynb'   coach_repos_zip.zip\r\n",
      " LM_Tuning_BigBird.ipynb\t        coach_repos_zip.zipranvcygstmp\r\n",
      " __MACOSX\t\t\t        filtered_repos.zip\r\n",
      " android.zip\t\t\t        intersection.py\r\n",
      " android2\t\t\t        logs_output\r\n",
      " artifacts\t\t\t        model_output\r\n",
      " bigbird_tuning.py\t\t        new_directory\r\n",
      " catboost_info\t\t\t        project_titles.pkl\r\n",
      " ck_results_of_repos.csv\t        train_data\r\n",
      " coach_repos_zip\t\t        train_data_sample_50percent\r\n"
     ]
    }
   ],
   "source": [
    "# from collections import defaultdict\n",
    "# project_wise_counts = defaultdict(list)\n",
    "\n",
    "# for _, (project, path) in df_all.iterrows():\n",
    "#     count = count_tokens_len(read_code(path))\n",
    "#     project_wise_counts[project].append(count[1])\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1618c19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/gcinizwe/miniconda3/envs/venv_dpd_py38/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BigBirdForMaskedLM, BigBirdTokenizer, BigBirdConfig, DataCollatorForLanguageModeling\n",
    "from transformers import LineByLineTextDataset, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained model\n",
    "model_name = \"google/bigbird-roberta-base\"\n",
    "config = BigBirdConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = BigBirdForMaskedLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Load dataset\n",
    "train_file = \"train_data_sample_50percent/train_data.txt\"\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_file,\n",
    "    block_size=config.block_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99eb7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = \"train_data_sample_50percent/test_data.txt\"\n",
    "eval_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=eval_file,\n",
    "    block_size=config.block_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "633f03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d014915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mkdir: cannot create directory â€˜model_output_sampleâ€™: File exists\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mkdir: cannot create directory â€˜logs_output_sampleâ€™: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir model_output_sample\n",
    "!mkdir logs_output_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e5961b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "output_dir = \"./model_output_sample\" \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=1_000,\n",
    "    save_total_limit=5,\n",
    "    logging_dir=\"./logs_output_sample\",\n",
    "    logging_steps=1_000,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a759a055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcinizwe/miniconda3/envs/venv_dpd_py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 69799\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 43625\n",
      "  Number of trainable parameters = 128111286\n",
      "You're using a BigBirdTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 64 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2565' max='43625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2565/43625 04:24 < 1:10:44, 9.67 it/s, Epoch 0.29/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.930600</td>\n",
       "      <td>1.300754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.227600</td>\n",
       "      <td>1.012144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7752\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model_output_sample/checkpoint-1000\n",
      "Configuration saved in ./model_output_sample/checkpoint-1000/config.json\n",
      "Model weights saved in ./model_output_sample/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7752\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model_output_sample/checkpoint-2000\n",
      "Configuration saved in ./model_output_sample/checkpoint-2000/config.json\n",
      "Model weights saved in ./model_output_sample/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_dpd_py38/lib/python3.8/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_dpd_py38/lib/python3.8/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_dpd_py38/lib/python3.8/site-packages/transformers/trainer.py:2526\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_dpd_py38/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_dpd_py38/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b30e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "LM_Tuning_BigBird.ipynb  filtered_repos.zip  model_output\t train_data\n",
      "android.zip\t\t intersection.py     new_directory\n",
      "android2\t\t logs_output\t     project_titles.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47674b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba103d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3510f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dpd_38",
   "language": "python",
   "name": "venv_dpd_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
